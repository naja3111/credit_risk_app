{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Model libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier # Use XGBClassifier as we have binary classification problem\n",
    "\n",
    "# Preprocessing and Pipeline libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "## model accuracy, score and cross validation libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Do not print warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a different approach - Fun with data wrangling\n",
    "\n",
    "*Note- Since there hand selected features (10) are available, skipping feature engineering part*\n",
    "\n",
    "* Combine and shuffle training and test data\n",
    "* create a 80-20 train and test split\n",
    "* Run a cross validation on the training set - This will help to avoid overfitting and underfitting\n",
    "* Build a model using XGBoost for credit risk prediction\n",
    "* Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'mle-project-challenge\\data\\combined_data.csv'  # Check with your folder path coreect them as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "There are  117911 records in dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading data...\")\n",
    "# load data\n",
    "data = pd.read_csv(data_path, engine='python', header=0)\n",
    "print(\"\\nThere are \", len(data), \"records in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate X, y and cretae Train Test Split (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out X and y\n",
    "X = data.loc[:, data.columns != 'is_late']\n",
    "y = data.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Sklearn train_test_split function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94328"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Presprocessing (borrow from what has been already done/given)\n",
    "###### We have mixed types, therefore, it has a nice pipeline to impute missing data and categorical transformers with onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign numeric features from dataset and impute with median for missing data.\n",
    "numeric_features = ['loan_amnt', \n",
    "                    'int_rate', 'annual_inc', 'revol_util', \n",
    "                    'dti', 'delinq_2yrs'\n",
    "                   ]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ])\n",
    "# Assign values from dataset\n",
    "categorical_features = ['purpose','grade', 'emp_length', 'home_ownership']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "## Main pre-preprocess variable\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pipeline and Fit XGBoost Model into Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pipeline\n",
    "# model = make_pipeline(preprocess, XGBClassifier())\n",
    "model = make_pipeline(preprocess, RandomForestClassifier(n_estimators = 100, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='median',\n",
       "                                                                                 verbose=0))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['loan_amnt', 'int_rate'...\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=3,\n",
       "                               min_child_weight=1, missing=None,\n",
       "                               n_estimators=100, n_jobs=1, nthread=None,\n",
       "                               objective='binary:logistic', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               seed=None, silent=None, subsample=1,\n",
       "                               verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train the model\n",
    "print(\"\\nTraining model ...\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9777382012466608\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\\n%s\" % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving model ...\")\n",
    "file = open('mle-project-challenge/models/credit_risk_model.pkl', 'wb')\n",
    "pickle.dump(model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function to Create Different Versions of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_model(number_of_estimators = 10):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    # Model libraries\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from xgboost import XGBClassifier # Use XGBClassifier as we have binary classification problem\n",
    "\n",
    "    # Preprocessing and Pipeline libraries\n",
    "    import sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    import pickle\n",
    "\n",
    "    ## model accuracy, score and cross validation libraries\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    # Do not print warning\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    cur_dir = os.getcwd() + \"\\\\\" + \"mle-project-challenge\"+ \"\\\\\"+ \"data\"\n",
    "    file_name = 'combined_data.csv'\n",
    "\n",
    "    for dirs, fol, files in os.walk(cur_dir):\n",
    "        for file in files:  \n",
    "            if file == file_name:\n",
    "                file_path = os.path.join(dirs, file_name)\n",
    "                print(file_path)\n",
    "    \n",
    "    # Load data into the python memory\n",
    "    data = pd.read_csv(file_path, engine='python', header=0)\n",
    "    \n",
    "    # Split data into X, y\n",
    "    X = data.loc[:, data.columns != 'is_late']\n",
    "    y = data.iloc[:,-1:]\n",
    "    \n",
    "    ## From Sklearn train_test_split function\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    ## Data Pre-Processing\n",
    "    # Assign numeric features from dataset and impute with median for missing data.\n",
    "    numeric_features = ['loan_amnt', \n",
    "                        'int_rate', 'annual_inc', 'revol_util', \n",
    "                        'dti', 'delinq_2yrs'\n",
    "                       ]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ])\n",
    "    # Assign values from dataset\n",
    "    categorical_features = ['purpose','grade', 'emp_length', 'home_ownership']\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "    ## Main pre-preprocess variable\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ## create a model pipeline with pre-processing\n",
    "    \n",
    "    print(\"deafault number of estimators of the model is : 100\")    \n",
    "    user_estimators = input(\"Do you want to change the estimators? if yes, enter a an integer number:\")\n",
    "    \n",
    "    if user_estimators != '':\n",
    "        user_estimators_int = int(user_estimators)\n",
    "        model = make_pipeline(preprocess, XGBClassifier(n_estimators=user_estimators_int))\n",
    "        \n",
    "    else:\n",
    "        model = make_pipeline(preprocess, XGBClassifier(n_estimators=number_of_estimators))\n",
    "        \n",
    "    \n",
    "    \n",
    "    ### Train the model\n",
    "    print(\"\\nTraining model ...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Print model accuracy\n",
    "    print(\"Accuracy:\\n%s\" % accuracy_score(y_test, model.predict(X_test)))\n",
    "    \n",
    "    save_path = os.getcwd() + \"\\\\\" + \"mle-project-challenge\"+ \"\\\\\"+ \"models\"\n",
    "    \n",
    "    version = int(input(\"please enter a model version to save the model:\"))\n",
    "    \n",
    "    if version != '':\n",
    "        int_version = int(version)\n",
    "        # Save model\n",
    "        print(\"\\nSaving model ...\")\n",
    "        file = open(save_path +\"//\"+'credit_risk_model_{}.pkl'.format(int_version), 'wb')\n",
    "        pickle.dump(model, file)\n",
    "        file.close()\n",
    "        print(\"Done writing file\")\n",
    "    \n",
    "    else:\n",
    "        # Save model\n",
    "        print(\"\\nSaving model ...\")\n",
    "        file = open(save_path +\"//\"+'credit_risk_model.pkl', 'wb')\n",
    "        pickle.dump(model, file)\n",
    "        file.close()\n",
    "        print(\"Done writing file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NajaMohamed\\Documents\\MLE_Challenge_Project\\mle-project-challenge\\data\\combined_data.csv\n",
      "deafault number of estimators of the model is : 100\n",
      "Do you want to change the estimators? if yes, enter a an integer number:\n",
      "\n",
      "Training model ...\n",
      "Accuracy:\n",
      "0.9777382012466608\n",
      "please enter a model version to save the model:1\n",
      "\n",
      "Saving model ...\n",
      "Done writing file\n"
     ]
    }
   ],
   "source": [
    "create_new_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
